{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82e6485-2d54-46df-9871-9ea8ae10b9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "# Fix the random seed to facilitate grading\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb7d468-d1f8-429b-99d7-d1568ae54527",
   "metadata": {},
   "source": [
    "# HW2.1 - Solvers for constrained optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa00ec6-4b6f-4b15-9310-6dbaf5cbbc1a",
   "metadata": {},
   "source": [
    "## 1.a Optimality conditions (25 pts)\n",
    "\n",
    "We start by creating a similar problem as in HW3-1, but adding linear constraints to it. We will see in the next notebook how we can solve a sequence of such programs to solve real-world problems such as the inverse kinematics problem of Practical 2. \n",
    "\n",
    "Our example will be\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min \\, & x^\\top Q x \\\\\n",
    "\\text{s.t.}\\, & A x = b\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefd4673",
   "metadata": {},
   "source": [
    "**1.a.1** Below's code is broken. Using the generated plot, explain in a couple of sentences why there must be a bug in your code. (4 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8187b429-60b2-4165-b218-9b749723e8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 2\n",
    "\n",
    "Q = np.random.rand(d, d)\n",
    "Q = Q.T @ Q\n",
    "\n",
    "A = np.full((1, d), 0.2)\n",
    "b = np.ones((1, 1))\n",
    "\n",
    "def f(x, Q):\n",
    "    return 0.5 * x.T @ Q @ x\n",
    "\n",
    "def g(x, A, b):\n",
    "    return A @ x - b\n",
    "\n",
    "def f_grad(x, Q):\n",
    "    return Q @ x\n",
    "\n",
    "def g_jac(x, A, b):\n",
    "    return A\n",
    "\n",
    "print(\"A =\", A)\n",
    "print(\"b =\", b)\n",
    "print(\"Q =\", Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005c6c04-94a9-47aa-b53d-425b06aa0ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_plot(Q, A, b, title=\"\"):\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(10, 10)\n",
    "\n",
    "    x = np.linspace(-10, 10, 200)\n",
    "    y = np.linspace(-14, 6, 200)\n",
    "    xx, yy = np.meshgrid(x, y)\n",
    "    zz = np.array([f(np.array([xi, yi]), Q) for xi, yi in zip(xx.flatten(), yy.flatten())])\n",
    "    \n",
    "    ax.contourf(xx, yy, zz.reshape(xx.shape), levels=30)\n",
    "\n",
    "    for a_i, b_i in zip(A, b):\n",
    "        a_x, a_y = a_i\n",
    "        ys = 1 / a_y * (b_i - a_x * x)\n",
    "        ax.plot(x, ys, color=\"C1\", label=\"g(x)=0\")\n",
    "        \n",
    "    ax.set_title(title)\n",
    "    ax.axis(\"equal\")\n",
    "    ax.set_ylim([-14, 6])\n",
    "    return fig, ax\n",
    "\n",
    "KKT_mat = np.hstack([np.vstack([Q, A]), np.vstack([A.T, np.zeros((A.shape[0], A.shape[0]))])])\n",
    "KKT_vec = np.vstack([np.zeros((Q.shape[0], 1)), -b]) \n",
    "z_opt = np.linalg.solve(KKT_mat, KKT_vec)\n",
    "x_opt, l_opt = z_opt[:d], z_opt[d:]\n",
    "print(x_opt)\n",
    "fig, ax = setup_plot(Q, A, b)\n",
    "ax.scatter(x_opt[0], x_opt[1], color=\"white\", marker=\"x\", label=\"optimum\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d761b9a-e1c1-44ea-b295-007d8a24c87d",
   "metadata": {},
   "source": [
    "**Answer**:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eaa4344-04e3-4126-8e95-1e9a1c1ba782",
   "metadata": {},
   "source": [
    "**1.a.2** Using your observations above, implement corresponding unit tests and make sure that they fail. (6 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c2a441-7e4c-48de-a224-2155869f7733",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ============= 1.a.2 Fill in below ==============\n",
    "### Find the expression of the optimal solution using the Lagrangian\n",
    "### and plot it. Explain below in words why this is an optimal solution. \n",
    "### Note that you can use the following structure to assert a test fails:\n",
    "### try:\n",
    "###    test_your_code()\n",
    "### except AssertionError as e:\n",
    "###    print(\"xx test failed!\")\n",
    "### else:\n",
    "###    print(\"xx test passed!\")\n",
    "\n",
    "def test_feasibility(x_opt, A, b):\n",
    "    NotImplementedError\n",
    "    \n",
    "def test_stationarity(x_opt, l_opt, Q, A, b):\n",
    "    NotImplementedError\n",
    "   \n",
    "def test_feasible_cost(x_opt, Q, A, b):\n",
    "    NotImplementedError\n",
    "\n",
    "# test the candidate solution\n",
    "try:\n",
    "    test_feasible_cost(x_opt, Q, A, b)\n",
    "except AssertionError as e:\n",
    "    print(\"cost test failed!\")\n",
    "else:\n",
    "    print(\"cost test passed!\")\n",
    "\n",
    "try:\n",
    "    test_feasibility(x_opt, A, b)\n",
    "except AssertionError as e:\n",
    "    print(\"feasibility test failed!\")\n",
    "else:\n",
    "    print(\"feasibility test passed!\")\n",
    "    \n",
    "try:\n",
    "    test_stationarity(x_opt, l_opt, Q=Q, A=A, b=b)\n",
    "except AssertionError as e:\n",
    "    print(\"stationarity test failed!\")\n",
    "else:\n",
    "    print(\"stationarity test passed!\")\n",
    "### ==========================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8fef78-c81f-423e-b806-b7bb97891c6b",
   "metadata": {},
   "source": [
    "**1.a.3** Now, fix the code and show that the tests pass. (5 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9bb30b-1ee7-4269-af43-db6f6d6d129a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ============= 1.a.3 Fill in below ==============\n",
    "### Copy over the code from 1.a.1 and fix it. Show that the tests pass. \n",
    "\n",
    "x_opt, l_opt = None, None\n",
    "\n",
    "fig, ax = setup_plot(Q, A, b)\n",
    "ax.scatter(x_opt[0], x_opt[1], color=\"white\", marker=\"x\")\n",
    "plt.legend()\n",
    "\n",
    "# test the candidate solution\n",
    "test_feasible_cost(x_opt, Q, A, b)\n",
    "test_feasibility(x_opt, A, b)\n",
    "test_stationarity(x_opt, l_opt, Q, A, b)\n",
    "### ==========================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1c39de-2498-4af3-ba2e-eeab2fc0ad9e",
   "metadata": {},
   "source": [
    "**1.a.4** Temporarily change A and b to break your algorithm. Can you think of two different ways to break it? Copy the below cell to create two different failure cases. (4 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008db76f-2cba-429d-b954-50b46eda5f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ============= 1.a.4 ====================\n",
    "### Intentionally break your algorithm below. Make a copy of this cell\n",
    "### and create another failure case. Comment the behavior (you can simply \n",
    "### change the failure message, or you can comment below the cell). \n",
    "\n",
    "Afail = None\n",
    "bfail = None\n",
    "failure_message = \"put your failure message here\"\n",
    "\n",
    "### =================================\n",
    "\n",
    "try:\n",
    "    KKT_mat = np.hstack([np.vstack([Q, Afail]), np.vstack([Afail.T, np.zeros((Afail.shape[0], Afail.shape[0]))])])\n",
    "    KKT_vec = np.vstack([np.zeros((Q.shape[0], 1)), bfail]) \n",
    "    z_opt = np.linalg.solve(KKT_mat, KKT_vec)\n",
    "    x_opt, l_opt = z_opt[:d], z_opt[d:]\n",
    "except Exception as e: \n",
    "    print(failure_message)\n",
    "    print(\"Exception:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2ee061-debb-4ef8-a931-3d8bebf93e7d",
   "metadata": {},
   "source": [
    "**1.a.5** Finally, we change Q to the following. Do we still get the correct minimum? Comment on the behavior and what would be the correct solution? Is there a particular value for A that will change this behavior?  (6 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21658383-6595-4244-81a1-5e0fd3799079",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_new = np.zeros((d, d))\n",
    "Q_new[0, 0] = -1.0\n",
    "Q_new[1, 1] = -1e-10\n",
    "\n",
    "KKT_mat = np.hstack([np.vstack([Q_new, A]), np.vstack([A.T, np.zeros((A.shape[0], A.shape[0]))])])\n",
    "KKT_vec = np.vstack([np.zeros((Q_new.shape[0], 1)), b]) \n",
    "z_opt = np.linalg.solve(KKT_mat, KKT_vec)\n",
    "x_opt, l_opt = z_opt[:d], z_opt[d:]\n",
    "\n",
    "fig, ax = setup_plot(Q_new, A, b)\n",
    "ax.scatter(x_opt[0], x_opt[1], color=\"white\", marker=\"x\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183cc997-c1a7-4caf-bde8-82f769fd8991",
   "metadata": {},
   "source": [
    "**Answer**:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd7d863-3643-4477-bc4e-9653935d6aff",
   "metadata": {},
   "source": [
    "## 1.b Newton solver and convergence (26 pts)\n",
    "\n",
    "In practice, we usually don't have the globally optimal solution and so instead of finding the optimal solution in one shot, we use an iterative method. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490b407f",
   "metadata": {},
   "source": [
    "**1.b.1** (10 pts) Adopt your code from above to compute Newton updates `p_k, delta_k`, so that we can use the update equations:\n",
    "$$\n",
    "x_{k+1} = x_k + \\alpha p_k, \\quad \\lambda_{k+1} = \\lambda_k + \\alpha \\Delta_k\n",
    "$$\n",
    "\n",
    "Using this update, implement a simple damped Newton solver. In particular, \n",
    "- Use the following convergence criterion: $|| \\nabla L(x, \\lambda) ||_p \\leq \\epsilon$, where you can choose $p=\\infty$ or $p=2$. \n",
    "- Use the unit tests from above to ensure that your code runs correctly.\n",
    "- (optional): use the 2D plots from above to plot the convergence of the iterates; in particular, plot also the constraint gradients scaled by $\\lambda$, and if you want, the constraints of $f$. Observe the convergence.\n",
    "\n",
    "Discuss the convergence, in particular the final primal and dual residuals. \n",
    "\n",
    "**Answer**: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146b0823-a5c2-4072-946e-1804ce33bcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### ============= 1.b.1 Fill in below ==============\n",
    "### Create a damped  Newton solver for the above problem. \n",
    "def damped_newton_eq_qp(\n",
    "    x0, Q, A, b, tol=1e-10, max_iter=50, alpha=0.5, infeasible=False\n",
    "):\n",
    "\n",
    "    return x_opt, l_opt, res_pri_list, res_dual_list, x_list, l_list\n",
    "### =========================== \n",
    "\n",
    "# ---- run solver ----\n",
    "x_0 = np.array([0, 5])\n",
    "x_opt, l_opt, res_pri_list, res_dual_list, x_list, l_list = damped_newton_eq_qp(x_0, Q, A, b)\n",
    "print(\"x_opt:\", x_opt)\n",
    "print(\"l_opt:\", l_opt)\n",
    "print(\"final residuals:\", res_pri_list[-1], res_dual_list[-1])\n",
    "\n",
    "\n",
    "### ============= 1.b.1 Fill in below ==============\n",
    "### Test and plot output\n",
    "\n",
    "test_feasibility(x_opt, A, b)\n",
    "test_stationarity(x_opt, l_opt, Q, A, b)\n",
    "test_feasible_cost(x_opt, Q, A, b)\n",
    "\n",
    "### =========================== "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193b8233-a420-4f3e-ad2f-9f9913681ee5",
   "metadata": {},
   "source": [
    "**1.b.2** Adopt the functions from homework 1.3 to plot the convergence of your solver. \n",
    "For the left plot, instead of plotting the cost, plot the primal and dual residuals. For the right plot, you can still plot the log-log distance to the optimum at steps k vs. k+1. Then, answer the following questions: (8 pts)\n",
    "a) Why do you think do we plot the gradients now, but we didn't plot them for the neural network solvers?\n",
    "b) What is the convergence rate and why? \n",
    "\n",
    "**Answers**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79e3f98-17b5-4038-8328-459bd6277c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ============= 1.b.2 Fill in below ==============\n",
    "\n",
    "# solution:\n",
    "def plot_convergence_rate(res_pri_list, res_dual_list, x_list, l_list):    \n",
    "    raise NotImplementedError\n",
    "\n",
    "### ===========================\n",
    "\n",
    "plot_convergence_rate(res_pri_list, res_dual_list, x_list, l_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d94b57-87da-44fd-80a7-739e045aa60e",
   "metadata": {},
   "source": [
    "**1.b.3** Start your solver at an infeasible point and observe the convergence. If you haven't done so yet, make sure to adapt your code so that it works for infeasible starts. Comment on the behavior. Is the convergence as you expected? Does the solver favor feasibility or optimality? Commong on the dual residual convergence and on the convergence rate? (8 pts)\n",
    "\n",
    "**Answer**: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9561bb-2f62-4117-b9bc-d6ef644ad03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0 = np.array([-5, 0])\n",
    "\n",
    "### ============= 1.b.3 Fill in below ==============\n",
    "\n",
    "\n",
    "### ===========================\n",
    "\n",
    "fig, ax = setup_plot(Q, A, b)\n",
    "ax.scatter(x_opt[0], x_opt[1], color=\"white\", marker=\"x\")\n",
    "for x_i, l_i in zip(x_list, l_list):\n",
    "    ax.scatter(x_i[0], x_i[1], color=\"white\", marker=\"o\") \n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plot_convergence_rate(res_pri_list, res_dual_list, x_list, l_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb6d62c-6307-4599-9dc3-7b7bb8b01e2f",
   "metadata": {},
   "source": [
    "## 1.c Penalty and Augmented Lagrangian (16 pts)\n",
    "\n",
    "Now, let's see how the convergence changes when we use the Penalty or Augmented Lagrangian method instead. \n",
    "\n",
    "**1.c.1** Below you can find an implementation of the Quadratic Penalty Method. Discuss the behavior for the following parameters. (6 pts)\n",
    "- Can you explain the difference in behavior of x_opt and x_approx? \n",
    "- What happens when you set max_iter to 1000 and why?\n",
    "- What happens when you set rho to 100 and why?\n",
    "\n",
    "**Answer**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5633fc7a-fb12-4632-a81a-2fd616589388",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "x_opt = np.zeros(d)\n",
    "x_approx = np.zeros(d)\n",
    "\n",
    "alpha = 0.01\n",
    "eps = 0.05\n",
    "\n",
    "rho = 5.0\n",
    "\n",
    "data = []\n",
    "\n",
    "max_iter = 100\n",
    "max_iter_inner = 100\n",
    "for i in range(max_iter):    \n",
    "    # optimal solution\n",
    "    x_opt = np.linalg.solve(Q + rho *A.T @ A, rho * np.sum(A.T @ b, axis=1))\n",
    "        \n",
    "    # approximate argmin\n",
    "    for i_inner in range(max_iter_inner):\n",
    "        grad = f_grad(x_approx, Q) + rho * np.sum(g_jac(x_approx, A, b).T @ g(x_approx, A, b), axis=1)\n",
    "        cost = f(x_approx, Q) + rho / 2 * np.sum(g(x_approx, A, b)**2)\n",
    "        x_approx = x_approx - alpha * grad\n",
    "\n",
    "    data.append(\n",
    "        {\n",
    "            \"rho\": rho,\n",
    "            \"cost\": f(x_approx, Q),\n",
    "            \"dual residual\": np.max(np.abs(A @ x_approx - b)), \n",
    "            \"iter\": i,\n",
    "            \"method\": \"approx\",\n",
    "        }\n",
    "    )\n",
    "    data.append(\n",
    "        {\n",
    "            \"rho\": rho,\n",
    "            \"cost\": f(x_opt, Q),\n",
    "            \"dual residual\": np.max(np.abs(A @ x_opt - b)), \n",
    "            \"iter\": i,\n",
    "            \"method\": \"opt\",\n",
    "        }\n",
    "    )\n",
    "    # update rho\n",
    "    rho = (1 + eps) * rho\n",
    "    \n",
    "### ===========================\n",
    "df = pd.DataFrame(data)\n",
    "fig, ax = plt.subplots()\n",
    "sns.lineplot(df, ax=ax, x=\"iter\", y=\"dual residual\", hue=\"method\")\n",
    "ax.set_yscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a014400-ab71-41df-a418-87d1b3ef9c58",
   "metadata": {},
   "source": [
    "We see above that convergence is a bit finicky with the Penalty method. Next we will study if things get better when we use the Augmented Lagrangian instead. \n",
    "\n",
    "**I.c.2** Implement the Augmented Lagrangian method below. (10 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f47ef40-8cbf-4b75-b63f-4732aedd47a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "### ============= 1.c.2 Fill in below ==============\n",
    "\n",
    "x_opt = np.zeros(d)\n",
    "l_opt = np.zeros(1)\n",
    "x_approx = np.zeros(d)\n",
    "l_approx = np.zeros(1)\n",
    "\n",
    "alpha = 0.01\n",
    "rho = 5.0\n",
    "\n",
    "data = []\n",
    "\n",
    "max_iter = 100\n",
    "max_iter_inner = 100\n",
    "for i in range(max_iter):    \n",
    "\n",
    "    x_opt, l_opt = None, None\n",
    "    \n",
    "    # approximate argmin\n",
    "    x_approx = None\n",
    "    l_approx = None\n",
    "\n",
    "    data.append(\n",
    "        {\n",
    "            \"rho\": rho,\n",
    "            \"cost\": f(x_approx, Q),\n",
    "            \"dual residual\": np.max(np.abs(A @ x_approx - b)), \n",
    "            \"iter\": i,\n",
    "            \"method\": \"approx\",\n",
    "        }\n",
    "    )\n",
    "    data.append(\n",
    "        {\n",
    "            \"rho\": rho,\n",
    "            \"cost\": f(x_opt, Q),\n",
    "            \"dual residual\": np.max(np.abs(A @ x_opt - b)), \n",
    "            \"iter\": i,\n",
    "            \"method\": \"opt\",\n",
    "        }\n",
    "    )\n",
    "    \n",
    "### ===========================\n",
    "df = pd.DataFrame(data)\n",
    "fig, ax = plt.subplots()\n",
    "sns.lineplot(df, ax=ax, x=\"iter\", y=\"dual residual\", hue=\"method\")\n",
    "ax.set_yscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195690f3-f137-47f3-a483-b9837e9ccb92",
   "metadata": {},
   "source": [
    "## 1.d ADMM (16 pts)\n",
    "\n",
    "We consider $\\ell_1$ penalized linear regression, also known as the LASSO problem: [Wikipedia](https://en.wikipedia.org/wiki/Lasso_(statistics)). Given an output vector $b \\in \\mathbb{R}^n$, a matrix $A \\in \\mathbb{R}^{n \\times p}$ of predictor variables, and a tuning parameter $\\lambda \\ge 0$, the lasso estimate can be defined as\n",
    "\n",
    "$$\n",
    "\\min_{x \\in \\mathbb{R}^n} \\; \\frac{1}{2}\\|Ax - b\\|_2^2 + \\lambda \\|x\\|_1\n",
    "$$\n",
    "\n",
    "The first term $\\|Ax - b\\|_2^2$ measures how well the model fits the data, while the second term $\\|x\\|_1$ penalizes the absolute values of the coefficients. The $\\ell_1$ penalty encourages **sparsity**, meaning that many components of $x$ become exactly zero. As a result, LASSO performs both **regression** and **feature selection** simultaneously.\n",
    "\n",
    "Because the $\\ell_1$ norm is **non-smooth**, the objective function is no longer a standard quadratic program (QP), and the closed-form solution for least squares do not apply. In this question, we use the **Alternating Direction Method of Multipliers (ADMM)**, which is well suited for problems with non-smooth regularization terms and allows the optimization to be decomposed into simpler subproblems.\n",
    "\n",
    "We start by rewriting the problem using variable splitting (ADMM form)\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min_{x,z}\\quad & \\frac{1}{2}\\|Ax-b\\|_2^2 + \\lambda\\|z\\|_1 \\\\\n",
    "\\text{s.t.}\\quad & x = z\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**I.d.1** (16 pts) Implement the ADMM method below. In particular: \n",
    "\n",
    "- Use the convergence critera as discussed in [Boyd Ch. 3.3.1](https://canvas.cmu.edu/courses/51076/files/folder/04_Resources?preview=13869096)\n",
    "- Try the automatic update rule of $\\rho$ as discussed in [Boyd Ch. 3.4.1](https://canvas.cmu.edu/courses/51076/files/folder/04_Resources?preview=13869096)\n",
    "\n",
    "Discuss the performance: \n",
    "\n",
    "a) Why is it interesting to use ADMM here? \n",
    "\n",
    "b) Discuss the observed convergence rate.\n",
    "\n",
    "c) Does the automatic update rule for $\\rho$ help? \n",
    "\n",
    "**Anwers**: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ead84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "### ============= 1.d.1 Implement ADMM below ==============\n",
    "\n",
    "    \n",
    "### ==========================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1bd464-c0c9-40eb-87a7-65aa4ae6f8aa",
   "metadata": {},
   "source": [
    "# 1.e Packaging of functions (10 pts)\n",
    "\n",
    "Just like in Homework 1, you will package what you wrote above as an easy-to-use python package for the next practical. \n",
    "\n",
    "**1.e.1** Create a python package, for example ``myoptim''. Move the relevant functions to this package, and create appropriate tests. After doing this, you should be able to install the package using `pip install -e myoptim`. Then make sure you create some tests and show their outputs, as in the example below. (5 pts)\n",
    "\n",
    "List of solvers for QP problem that need to move to this package:\n",
    "- Damped Newton solver \n",
    "- Augmented Lagrangian solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b799a6fb-4c76-4c9f-bdab-eeaae50ea77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest myoptim/tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe50ea1-b4ce-4468-a41c-b431ec51f306",
   "metadata": {},
   "source": [
    "**1.e.2** Answer the questions below about the package that you created. Note that there is no right or wrong; these questions serve to make you reflect about what you have implemented. (5 pts)\n",
    "- What tests did you implement? Explain each test with one short sentence.\n",
    "- Are you sure that your code works based on these tests? Reflect on whether all crucial aspects of the code are tested. What could still go wrong? \n",
    "- Is the interface to your code straightforward (i.e., how many lines of code are required to run the solvers? Would it be easy for someone to use it? A good sanity check is if your code is modular so that many functions can be easily tested.) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0e9e89-80b8-4661-923b-2739daaa8831",
   "metadata": {},
   "source": [
    "## Acknowledgment of Collaboration and/or Tool Use\n",
    "\n",
    "Please choose from below (simply delete the lines that do not apply) and add a few additional notes\n",
    "\n",
    "- “I worked alone on this assignment.”, or\n",
    "- “I worked with ~~~~~~ [person or tool] on this assignment.” and/or\n",
    "- “I received assistance from ~~~~~~ [person or tool] on this assignment.”\n",
    "\n",
    "For the last two cases, specify how the person or tool helped you and explain why this amplified your learning process:\n",
    "\n",
    "_add answer here_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1589e860-9291-44b0-b96a-6b1bf8f5a0b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
